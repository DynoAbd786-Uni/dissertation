{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393be214",
   "metadata": {},
   "source": [
    "# Simulation VTK Data Processor\n",
    "\n",
    "This notebook loads .vtk files from LBM simulations (aneurysm, pipe, etc.), extracts the specific fields (rho, velocity, wall shear stress, etc.), and organizes them into dictionaries of NumPy arrays for further analysis and visualization. Each field will be accessible by frame number.\n",
    "\n",
    "This version has been modified to automatically search for VTK files recursively starting from one directory above the current working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4647d736",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7902ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import vtk\n",
    "from vtk.util import numpy_support\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac7da2",
   "metadata": {},
   "source": [
    "## Recursively Search for VTK File Collections\n",
    "\n",
    "Starting from one directory above the current working directory, we'll search for all VTK file collections and organize them by pipe configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "721548b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting search from: /home/abdua786/code/uni/3/dissertation/dissertation/results\n",
      "Found VTK collection in: /home/abdua786/code/uni/3/dissertation/dissertation/results/pipe_flow/time_dependent_zouhe_non_newtonian_bgk/vtk\n",
      "  Config directory: /home/abdua786/code/uni/3/dissertation/dissertation/results/pipe_flow/time_dependent_zouhe_non_newtonian_bgk\n",
      "  Number of VTK files: 101\n",
      "Found VTK collection in: /home/abdua786/code/uni/3/dissertation/dissertation/results/pipe_flow/standard_zouhe_standard_bgk/vtk\n",
      "  Config directory: /home/abdua786/code/uni/3/dissertation/dissertation/results/pipe_flow/standard_zouhe_standard_bgk\n",
      "  Number of VTK files: 101\n",
      "Found VTK collection in: /home/abdua786/code/uni/3/dissertation/dissertation/results/pipe_flow/standard_zouhe_non_newtonian_bgk/vtk\n",
      "  Config directory: /home/abdua786/code/uni/3/dissertation/dissertation/results/pipe_flow/standard_zouhe_non_newtonian_bgk\n",
      "  Number of VTK files: 101\n",
      "Found VTK collection in: /home/abdua786/code/uni/3/dissertation/dissertation/results/pipe_flow/time_dependent_zouhe_standard_bgk/vtk\n",
      "  Config directory: /home/abdua786/code/uni/3/dissertation/dissertation/results/pipe_flow/time_dependent_zouhe_standard_bgk\n",
      "  Number of VTK files: 101\n",
      "Found VTK collection in: /home/abdua786/code/uni/3/dissertation/dissertation/results/aneurysm_flow/CCA_simulation_results_nnbgk_tdzh/vtk\n",
      "  Config directory: /home/abdua786/code/uni/3/dissertation/dissertation/results/aneurysm_flow/CCA_simulation_results_nnbgk_tdzh\n",
      "  Number of VTK files: 101\n",
      "\n",
      "Found 5 VTK collections to process:\n",
      "1. time_dependent_zouhe_non_newtonian_bgk - 101 files\n",
      "2. standard_zouhe_standard_bgk - 101 files\n",
      "3. standard_zouhe_non_newtonian_bgk - 101 files\n",
      "4. time_dependent_zouhe_standard_bgk - 101 files\n",
      "5. CCA_simulation_results_nnbgk_tdzh - 101 files\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory and go one directory up\n",
    "current_directory = Path.cwd()\n",
    "parent_directory = current_directory.parent / 'results'\n",
    "\n",
    "print(f\"Starting search from: {parent_directory}\")\n",
    "\n",
    "# Function to find VTK collections recursively\n",
    "def find_vtk_collections(root_dir):\n",
    "    vtk_collections = []\n",
    "    \n",
    "    # Walk through all directories under the root\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        path = Path(dirpath)\n",
    "        \n",
    "        # Check if this directory contains VTK files\n",
    "        vtk_files = list(path.glob('*.vtk'))\n",
    "        if vtk_files:\n",
    "            # Found a collection of VTK files\n",
    "            vtk_collections.append({\n",
    "                'vtk_dir': path,\n",
    "                'config_dir': path.parent,  # Directory containing the VTK collection is the pipe config\n",
    "                'vtk_files': vtk_files\n",
    "            })\n",
    "            print(f\"Found VTK collection in: {path}\")\n",
    "            print(f\"  Config directory: {path.parent}\")\n",
    "            print(f\"  Number of VTK files: {len(vtk_files)}\")\n",
    "    \n",
    "    return vtk_collections\n",
    "\n",
    "# Find all VTK collections\n",
    "vtk_collections = find_vtk_collections(parent_directory)\n",
    "\n",
    "if not vtk_collections:\n",
    "    print(\"Error: No VTK collections found.\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(vtk_collections)} VTK collections to process:\")\n",
    "    for i, collection in enumerate(vtk_collections):\n",
    "        print(f\"{i+1}. {collection['config_dir'].name} - {len(collection['vtk_files'])} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0289e",
   "metadata": {},
   "source": [
    "## Define Processing Functions\n",
    "\n",
    "Here we define all the functions needed to process VTK files and convert them to NPZ format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daf57cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_sort_key(s):\n",
    "    \"\"\"Sort strings with embedded numbers naturally\"\"\"\n",
    "    return [int(c) if c.isdigit() else c.lower() for c in re.split(r'(\\d+)', str(s))]\n",
    "\n",
    "def extract_frame_number(filename):\n",
    "    \"\"\"Extract the frame number from the VTK filename\"\"\"\n",
    "    # Extract all digits from the filename\n",
    "    digits = ''.join(re.findall(r'\\d+', filename))\n",
    "    if digits:\n",
    "        return int(digits)\n",
    "    return None\n",
    "\n",
    "def read_vtk_file(file_path):\n",
    "    \"\"\"Read a VTK file and return the data object\"\"\"\n",
    "    reader = vtk.vtkStructuredPointsReader()\n",
    "    reader.SetFileName(str(file_path))\n",
    "    reader.Update()\n",
    "    return reader.GetOutput()\n",
    "\n",
    "def extract_vtk_fields(vtk_data):\n",
    "    \"\"\"Extract all fields from a VTK data object\"\"\"\n",
    "    fields = {}\n",
    "    \n",
    "    # Extract cell data (where most of our fields are located)\n",
    "    cell_data = vtk_data.GetCellData()\n",
    "    num_arrays = cell_data.GetNumberOfArrays()\n",
    "    \n",
    "    for i in range(num_arrays):\n",
    "        array_name = cell_data.GetArrayName(i)\n",
    "        vtk_array = cell_data.GetArray(array_name)\n",
    "        numpy_array = numpy_support.vtk_to_numpy(vtk_array)\n",
    "        fields[array_name] = numpy_array\n",
    "    \n",
    "    # Extract point data (if any)\n",
    "    point_data = vtk_data.GetPointData()\n",
    "    num_point_arrays = point_data.GetNumberOfArrays()\n",
    "    \n",
    "    for i in range(num_point_arrays):\n",
    "        array_name = point_data.GetArrayName(i)\n",
    "        vtk_array = point_data.GetArray(array_name)\n",
    "        numpy_array = numpy_support.vtk_to_numpy(vtk_array)\n",
    "        fields[f\"point_{array_name}\"] = numpy_array\n",
    "    \n",
    "    # Store dimensions for future reference\n",
    "    dims = vtk_data.GetDimensions()\n",
    "    fields['dimensions'] = np.array(dims)\n",
    "    \n",
    "    return fields\n",
    "\n",
    "def process_all_vtk_files(vtk_files, max_files=None):\n",
    "    \"\"\"Process all VTK files and organize their fields by name and frame\"\"\"\n",
    "    # Initialize dictionaries to store each field\n",
    "    field_data = {}\n",
    "    frame_numbers = []\n",
    "    \n",
    "    # Limit the number of files to process if specified\n",
    "    if max_files is not None and max_files > 0:\n",
    "        vtk_files = vtk_files[:max_files]\n",
    "    \n",
    "    # Process each VTK file\n",
    "    for file_path in tqdm.tqdm(vtk_files, desc=\"Processing VTK files\"):\n",
    "        try:\n",
    "            # Extract frame number from filename\n",
    "            frame_num = extract_frame_number(file_path.name)\n",
    "            if frame_num is None:\n",
    "                continue\n",
    "                \n",
    "            frame_numbers.append(frame_num)\n",
    "            \n",
    "            # Read and extract fields\n",
    "            vtk_data = read_vtk_file(file_path)\n",
    "            fields = extract_vtk_fields(vtk_data)\n",
    "            \n",
    "            # Organize fields by name, with frames stored sequentially\n",
    "            for field_name, field_array in fields.items():\n",
    "                if field_name not in field_data:\n",
    "                    field_data[field_name] = {}\n",
    "                \n",
    "                field_data[field_name][frame_num] = field_array\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {e}\")\n",
    "    \n",
    "    # Get list of unique field names\n",
    "    all_fields = list(field_data.keys())\n",
    "    print(f\"\\nProcessed {len(frame_numbers)} VTK files with {len(all_fields)} fields per file\")\n",
    "    print(f\"Field names: {', '.join(all_fields)}\")\n",
    "    \n",
    "    return field_data, sorted(frame_numbers)\n",
    "\n",
    "def stack_fields_into_arrays(field_data, frame_numbers):\n",
    "    \"\"\"Stack all frames for each field into 3D NumPy arrays\"\"\"\n",
    "    stacked_fields = {}\n",
    "    \n",
    "    for field_name, frames_data in field_data.items():\n",
    "        if field_name == 'dimensions':\n",
    "            # Just store one copy of the dimensions\n",
    "            stacked_fields[field_name] = next(iter(frames_data.values()))\n",
    "            continue\n",
    "        \n",
    "        # Check if all frames have the same shape\n",
    "        shapes = {frame: data.shape for frame, data in frames_data.items()}\n",
    "        if len(set(shapes.values())) > 1:\n",
    "            print(f\"Warning: Field '{field_name}' has inconsistent shapes across frames\")\n",
    "            stacked_fields[field_name] = frames_data  # Store as dictionary\n",
    "            continue\n",
    "        \n",
    "        # All frames have the same shape, stack them into a 3D array\n",
    "        try:\n",
    "            # Create arrays in order of frame numbers\n",
    "            sorted_frames = sorted(frames_data.keys())\n",
    "            arrays_to_stack = [frames_data[frame] for frame in sorted_frames]\n",
    "            \n",
    "            # Stack along a new first dimension (frame)\n",
    "            stacked_array = np.stack(arrays_to_stack, axis=0)\n",
    "            \n",
    "            # Store the stacked array and frame mapping\n",
    "            stacked_fields[field_name] = stacked_array\n",
    "            \n",
    "            print(f\"Stacked '{field_name}' into array with shape {stacked_array.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error stacking {field_name}: {e}\")\n",
    "            # If stacking fails, store as a dictionary\n",
    "            stacked_fields[field_name] = frames_data\n",
    "    \n",
    "    # Create a mapping from frame numbers to indices\n",
    "    frame_to_index = {frame: i for i, frame in enumerate(sorted(frame_numbers))}\n",
    "    \n",
    "    return stacked_fields, frame_to_index\n",
    "\n",
    "def reshape_all_fields_to_2d(stacked_fields, frame_to_index=None):\n",
    "    \"\"\"Reshape all 1D field data into 2D arrays for easier analysis and visualization.\"\"\"\n",
    "    if 'dimensions' not in stacked_fields:\n",
    "        print(\"Dimensions not available for reshaping data\")\n",
    "        return stacked_fields\n",
    "    \n",
    "    # Get dimensions for reshaping\n",
    "    dimensions = stacked_fields['dimensions']\n",
    "    real_dims = [d for d in dimensions if d > 1]\n",
    "    \n",
    "    if len(real_dims) < 2:\n",
    "        print(\"Cannot reshape data - insufficient dimensions\")\n",
    "        return stacked_fields\n",
    "    \n",
    "    # Dimensions for reshaping - need to subtract 1 because VTK dimensions are cell counts + 1\n",
    "    ny, nx = real_dims[1]-1, real_dims[0]-1\n",
    "    print(f\"Reshaping fields to dimensions: ({ny}, {nx})\")\n",
    "    \n",
    "    # Create a new dictionary to store the reshaped fields\n",
    "    reshaped_fields = {'dimensions': stacked_fields['dimensions']}\n",
    "    \n",
    "    # Process each field\n",
    "    for field_name, field_data in stacked_fields.items():\n",
    "        if field_name == 'dimensions':\n",
    "            continue\n",
    "            \n",
    "        print(f\"Reshaping field: {field_name}\")\n",
    "        \n",
    "        # Handle 3D arrays (stacked frames)\n",
    "        if isinstance(field_data, np.ndarray) and field_data.ndim > 1:\n",
    "            # Get the number of frames\n",
    "            n_frames = field_data.shape[0]\n",
    "            \n",
    "            # Create a new array with reshaped dimensions\n",
    "            reshaped_array = np.zeros((n_frames, ny, nx), dtype=field_data.dtype)\n",
    "            \n",
    "            # Reshape each frame\n",
    "            for i in range(n_frames):\n",
    "                frame_data = field_data[i]\n",
    "                # Reshape the 1D array to 2D\n",
    "                try:\n",
    "                    reshaped_array[i] = np.flipud(frame_data.reshape(ny, nx))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reshaping frame {i} for {field_name}: {e}\")\n",
    "                    # Keep original data if reshaping fails\n",
    "                    reshaped_array[i] = frame_data\n",
    "            \n",
    "            # Store the reshaped array\n",
    "            reshaped_fields[field_name] = reshaped_array\n",
    "            \n",
    "        # Handle dictionary of frames\n",
    "        elif isinstance(field_data, dict):\n",
    "            reshaped_dict = {}\n",
    "            \n",
    "            # Reshape each frame in the dictionary\n",
    "            for frame_num, frame_data in field_data.items():\n",
    "                try:\n",
    "                    reshaped_dict[frame_num] = frame_data.reshape(ny, nx)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reshaping frame {frame_num} for {field_name}: {e}\")\n",
    "                    # Keep original data if reshaping fails\n",
    "                    reshaped_dict[frame_num] = frame_data\n",
    "            \n",
    "            # Store the reshaped dictionary\n",
    "            reshaped_fields[field_name] = reshaped_dict\n",
    "        # Handle single arrays (not per-frame)\n",
    "        elif isinstance(field_data, np.ndarray) and field_data.ndim == 1 and field_name != 'dimensions':\n",
    "            try:\n",
    "                reshaped_fields[field_name] = field_data.reshape(ny, nx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reshaping {field_name}: {e}\")\n",
    "                # Keep original data if reshaping fails\n",
    "                reshaped_fields[field_name] = field_data\n",
    "        else:\n",
    "            # For any other type of data, keep as is\n",
    "            reshaped_fields[field_name] = field_data\n",
    "    \n",
    "    print(f\"Reshaped {len(reshaped_fields)-1} fields (excluding dimensions)\")\n",
    "    \n",
    "    # Apply vertical flip correction to the reshaped data\n",
    "    print(\"\\nApplying vertical flip orientation correction to reshaped data...\")\n",
    "    corrected_fields = {'dimensions': reshaped_fields['dimensions']}\n",
    "    \n",
    "    for field_name, field_data in reshaped_fields.items():\n",
    "        if field_name == 'dimensions':\n",
    "            continue\n",
    "            \n",
    "        # Handle 3D arrays (stacked frames)\n",
    "        if isinstance(field_data, np.ndarray) and field_data.ndim == 3:\n",
    "            # Create a new array with the same shape\n",
    "            corrected_array = np.zeros_like(field_data)\n",
    "            \n",
    "            # Flip each frame\n",
    "            for i in range(field_data.shape[0]):\n",
    "                # Apply vertical flip\n",
    "                corrected_array[i] = np.flipud(field_data[i])\n",
    "                \n",
    "                # Negate velocity components if needed\n",
    "                if field_name in ['u_x', 'wss_x']:\n",
    "                    corrected_array[i] = -corrected_array[i]\n",
    "            \n",
    "            corrected_fields[field_name] = corrected_array\n",
    "            \n",
    "        # Handle dictionary of frames\n",
    "        elif isinstance(field_data, dict):\n",
    "            corrected_dict = {}\n",
    "            \n",
    "            for frame_num, frame_data in field_data.items():\n",
    "                if frame_data.ndim == 2:  # Only process 2D arrays\n",
    "                    corrected = np.flipud(frame_data)\n",
    "                    \n",
    "                    # Negate velocity components if needed\n",
    "                    if field_name in ['u_x', 'wss_x']:\n",
    "                        corrected = -corrected\n",
    "                        \n",
    "                    corrected_dict[frame_num] = corrected\n",
    "                else:\n",
    "                    corrected_dict[frame_num] = frame_data\n",
    "            \n",
    "            corrected_fields[field_name] = corrected_dict\n",
    "            \n",
    "        # Handle 2D arrays (single frame)\n",
    "        elif isinstance(field_data, np.ndarray) and field_data.ndim == 2:\n",
    "            corrected = np.flipud(field_data)\n",
    "            \n",
    "            # Negate velocity components if needed\n",
    "            if field_name in ['u_x', 'wss_x']:\n",
    "                corrected = -corrected\n",
    "                \n",
    "            corrected_fields[field_name] = corrected\n",
    "        else:\n",
    "            # For any other type of data, keep as is\n",
    "            corrected_fields[field_name] = field_data\n",
    "    \n",
    "    return corrected_fields\n",
    "\n",
    "def save_stacked_fields_as_npz(fields_to_save, frame_to_index, output_dir=None, simulation_folder=None):\n",
    "    \"\"\"Save each field from stacked_fields as individual .npz files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fields_to_save : dict\n",
    "        Dictionary containing the fields to save. These can be either the original stacked fields\n",
    "        or reshaped fields, depending on what's passed in.\n",
    "    frame_to_index : dict\n",
    "        Mapping from frame numbers to indices in the arrays.\n",
    "    output_dir : Path, optional\n",
    "        Directory to save the .npz files. If None, will save to processed_data/raw_fields in simulation_folder.\n",
    "    simulation_folder : Path, optional\n",
    "        The simulation folder containing the VTK files.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    import time\n",
    "    \n",
    "    # Create output directory based on simulation folder\n",
    "    if output_dir is None and simulation_folder is not None:\n",
    "        # Save to processed_data/raw_fields in the selected simulation folder\n",
    "        output_dir = simulation_folder / 'processed_data' / 'raw_fields'\n",
    "    elif output_dir is None:\n",
    "        # Fallback to standard location\n",
    "        base_dir = Path('/home/abdua786/code/uni/3/dissertation/dissertation')\n",
    "        output_dir = base_dir / 'processed_data' / 'raw_fields'\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving fields to {output_dir}\")\n",
    "    \n",
    "    # Convert frame_to_index to arrays for storage\n",
    "    frame_numbers = np.array(list(frame_to_index.keys()))\n",
    "    frame_indices = np.array(list(frame_to_index.values()))\n",
    "    \n",
    "    # Save frame mapping for reference\n",
    "    frame_mapping_file = output_dir / 'frame_mapping.npz'\n",
    "    np.savez_compressed(\n",
    "        frame_mapping_file, \n",
    "        frame_numbers=frame_numbers, \n",
    "        frame_indices=frame_indices\n",
    "    )\n",
    "    print(f\"Saved frame mapping to {frame_mapping_file}\")\n",
    "    \n",
    "    # Store paths of saved files\n",
    "    saved_files = {'frame_mapping': frame_mapping_file}\n",
    "    \n",
    "    # Also save the dimensions.npz file\n",
    "    if 'dimensions' in fields_to_save:\n",
    "        dimensions_file = output_dir / 'dimensions.npz'\n",
    "        np.savez_compressed(dimensions_file, data=fields_to_save['dimensions'])\n",
    "        saved_files['dimensions'] = dimensions_file\n",
    "        print(f\"Saved dimensions to {dimensions_file}\")\n",
    "    \n",
    "    # Save each field as a separate npz file\n",
    "    for field_name, field_data in fields_to_save.items():\n",
    "        if field_name == 'dimensions':\n",
    "            continue  # Already saved separately\n",
    "            \n",
    "        if isinstance(field_data, np.ndarray):\n",
    "            # Create filename based on field name\n",
    "            output_file = output_dir / f\"{field_name}.npz\"\n",
    "            \n",
    "            # Save as compressed npz file - each field gets its own file\n",
    "            np.savez_compressed(output_file, data=field_data)\n",
    "            \n",
    "            # Record file size for reporting\n",
    "            file_size = os.path.getsize(output_file) / (1024**2)  # size in MB\n",
    "            saved_files[field_name] = output_file\n",
    "            \n",
    "            print(f\"Saved field '{field_name}' to {output_file} ({file_size:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"Skipping field '{field_name}' as it's not a NumPy array\")\n",
    "    \n",
    "    # Create a README file with information about the data\n",
    "    readme_path = output_dir / \"README.txt\"\n",
    "    with open(readme_path, 'w') as f:\n",
    "        simulation_type = simulation_folder.name if simulation_folder is not None else \"Simulation\"\n",
    "        f.write(f\"{simulation_type} Field Data\\n\")\n",
    "        f.write(\"=\"* (len(simulation_type) + 11) + \"\\n\\n\")\n",
    "        f.write(f\"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Number of frames: {len(frame_numbers)}\\n\")\n",
    "        f.write(\"\\nAvailable fields:\\n\")\n",
    "        \n",
    "        for field_name in fields_to_save.keys():\n",
    "            if field_name in saved_files:\n",
    "                field_data = fields_to_save[field_name]\n",
    "                if isinstance(field_data, np.ndarray):\n",
    "                    shape_str = str(field_data.shape)\n",
    "                    f.write(f\"- {field_name}: {shape_str}\\n\")\n",
    "        \n",
    "        f.write(\"\\nNotes:\\n\")\n",
    "        # Check if any of the fields are 3D arrays (frames, height, width)\n",
    "        multi_dim = any(isinstance(data, np.ndarray) and data.ndim > 2 \n",
    "                        for field_name, data in fields_to_save.items() \n",
    "                        if field_name != 'dimensions')\n",
    "        if multi_dim:\n",
    "            f.write(\"- These files contain 3D arrays with dimensions [frames, height, width]\\n\")\n",
    "        else:\n",
    "            f.write(\"- These files contain the raw field data without reshaping\\n\")\n",
    "        f.write(\"- The frame_mapping.npz file contains the mapping between frame numbers and indices\\n\")\n",
    "        f.write(\"- Each field is stored as a separate .npz file for easier loading\\n\")\n",
    "        f.write(\"- To load a field: data = np.load('field_name.npz')['data']\\n\")\n",
    "    \n",
    "    print(f\"\\nSaved {len(saved_files)-2} fields to {output_dir}\")\n",
    "    print(f\"Created README at {readme_path}\")\n",
    "    \n",
    "    return saved_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b1a4e",
   "metadata": {},
   "source": [
    "## Automatic Processing Pipeline\n",
    "\n",
    "This cell will automatically process all VTK collections recursively, converting their VTK files to NPZ format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cb9542b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting automatic processing of 5 VTK collections\n",
      "\n",
      "================================================================================\n",
      "Processing VTK collection: /home/abdua786/code/uni/3/dissertation/dissertation/results/pipe_flow/time_dependent_zouhe_non_newtonian_bgk/vtk\n",
      "Config directory: time_dependent_zouhe_non_newtonian_bgk\n",
      "================================================================================\n",
      "\n",
      "Found 101 VTK files\n",
      "Detected simulation type: pipe_tdzh_nnbgk_\n",
      "Example VTK files:\n",
      "- pipe_tdzh_nnbgk_0000000.vtk\n",
      "- pipe_tdzh_nnbgk_0001000.vtk\n",
      "- pipe_tdzh_nnbgk_0002000.vtk\n",
      "... and 98 more files\n",
      "\n",
      "Processing VTK files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing VTK files:  78%|███████▊  | 79/101 [00:02<00:00, 27.62it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 145\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vtk_collections) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting automatic processing of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vtk_collections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m VTK collections\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 145\u001b[0m     processing_results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_all_vtk_collections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvtk_collections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_files_to_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo VTK collections found to process\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 116\u001b[0m, in \u001b[0;36mprocess_all_vtk_collections\u001b[0;34m(collections, max_files_to_process)\u001b[0m\n\u001b[1;32m    114\u001b[0m config_name \u001b[38;5;241m=\u001b[39m collection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    115\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 116\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_vtk_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_files_to_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    118\u001b[0m results[config_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m'\u001b[39m: success,\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessing_time\u001b[39m\u001b[38;5;124m'\u001b[39m: end_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    121\u001b[0m }\n",
      "Cell \u001b[0;32mIn[24], line 47\u001b[0m, in \u001b[0;36mprocess_vtk_collection\u001b[0;34m(collection, max_files_to_process)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLimiting to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_files_to_process\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Process the files\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m vtk_field_data, frame_numbers \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_all_vtk_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvtk_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_files_to_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Print information about the processed data\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessed frames: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(frame_numbers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(frame_numbers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 71\u001b[0m, in \u001b[0;36mprocess_all_vtk_files\u001b[0;34m(vtk_files, max_files)\u001b[0m\n\u001b[1;32m     68\u001b[0m frame_numbers\u001b[38;5;241m.\u001b[39mappend(frame_num)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Read and extract fields\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m vtk_data \u001b[38;5;241m=\u001b[39m \u001b[43mread_vtk_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m fields \u001b[38;5;241m=\u001b[39m extract_vtk_fields(vtk_data)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Organize fields by name, with frames stored sequentially\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 17\u001b[0m, in \u001b[0;36mread_vtk_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m reader \u001b[38;5;241m=\u001b[39m vtk\u001b[38;5;241m.\u001b[39mvtkStructuredPointsReader()\n\u001b[1;32m     16\u001b[0m reader\u001b[38;5;241m.\u001b[39mSetFileName(\u001b[38;5;28mstr\u001b[39m(file_path))\n\u001b[0;32m---> 17\u001b[0m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUpdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mGetOutput()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_vtk_collection(collection, max_files_to_process=None):\n",
    "    \"\"\"Process a single VTK collection and convert its VTK files to NPZ format\"\"\"\n",
    "    vtk_dir = collection['vtk_dir']\n",
    "    config_dir = collection['config_dir']\n",
    "    vtk_files = collection['vtk_files']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing VTK collection: {vtk_dir}\")\n",
    "    print(f\"Config directory: {config_dir.name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create processed_data directory within the config directory\n",
    "    processed_data_folder = config_dir / 'processed_data'\n",
    "    processed_data_folder.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Sort VTK files\n",
    "    vtk_files = sorted(vtk_files, key=natural_sort_key)\n",
    "    if not vtk_files:\n",
    "        print(f\"Error: No VTK files found in {vtk_dir}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Found {len(vtk_files)} VTK files\")\n",
    "    \n",
    "    # Extract simulation type (prefix) from the first VTK file\n",
    "    filename = vtk_files[0].name\n",
    "    match = re.match(r'([a-zA-Z_]+)\\d+', filename)\n",
    "    if match:\n",
    "        simulation_prefix = match.group(1)\n",
    "        print(f\"Detected simulation type: {simulation_prefix}\")\n",
    "    else:\n",
    "        simulation_prefix = \"simulation_\"\n",
    "        print(f\"Could not detect simulation prefix, using default: {simulation_prefix}\")\n",
    "    \n",
    "    # Display some example filenames\n",
    "    print(\"Example VTK files:\")\n",
    "    for file in vtk_files[:3]:  # Show first 3 files\n",
    "        print(f\"- {file.name}\")\n",
    "    if len(vtk_files) > 3:\n",
    "        print(f\"... and {len(vtk_files) - 3} more files\")\n",
    "    \n",
    "    # Process the VTK files\n",
    "    print(f\"\\nProcessing VTK files...\")\n",
    "    if max_files_to_process:\n",
    "        print(f\"Limiting to {max_files_to_process} files\")\n",
    "    \n",
    "    # Process the files\n",
    "    vtk_field_data, frame_numbers = process_all_vtk_files(vtk_files, max_files_to_process)\n",
    "    \n",
    "    # Print information about the processed data\n",
    "    print(f\"\\nProcessed frames: {min(frame_numbers)} to {max(frame_numbers)}\")\n",
    "    \n",
    "    # Check data dimensions\n",
    "    for field_name, frames_data in vtk_field_data.items():\n",
    "        if field_name != 'dimensions':\n",
    "            # Get the shape of the first frame's data\n",
    "            first_frame = frames_data[frame_numbers[0]]\n",
    "            print(f\"Field '{field_name}': {len(frames_data)} frames, shape per frame: {first_frame.shape}\")\n",
    "    \n",
    "    # Stack the fields\n",
    "    print(\"\\nStacking fields into 3D arrays...\")\n",
    "    stacked_fields, frame_to_index = stack_fields_into_arrays(vtk_field_data, frame_numbers)\n",
    "    \n",
    "    # Print information about the stacked data\n",
    "    print(\"\\nStacked fields:\")\n",
    "    for field_name, field_array in stacked_fields.items():\n",
    "        if isinstance(field_array, np.ndarray):\n",
    "            if field_name != 'dimensions':\n",
    "                print(f\"- {field_name}: shape {field_array.shape}, type {field_array.dtype}, memory usage {field_array.nbytes / (1024**2):.2f} MB\")\n",
    "            else:\n",
    "                print(f\"- {field_name}: {field_array}\")\n",
    "        else:\n",
    "            print(f\"- {field_name}: dictionary with {len(field_array)} frames\")\n",
    "    \n",
    "    # Reshape the fields\n",
    "    print(\"\\nReshaping all fields to 2D arrays...\")\n",
    "    reshaped_fields = reshape_all_fields_to_2d(stacked_fields, frame_to_index)\n",
    "    \n",
    "    # Print information about the reshaped data\n",
    "    print(\"\\nReshaped fields:\")\n",
    "    for field_name, field_array in reshaped_fields.items():\n",
    "        if isinstance(field_array, np.ndarray):\n",
    "            if field_name != 'dimensions':\n",
    "                print(f\"- {field_name}: shape {field_array.shape}, type {field_array.dtype}, memory usage {field_array.nbytes / (1024**2):.2f} MB\")\n",
    "            else:\n",
    "                print(f\"- {field_name}: {field_array}\")\n",
    "        else:\n",
    "            print(f\"- {field_name}: dictionary with {len(field_array)} frames\")\n",
    "    \n",
    "    # Save the reshaped fields (3D arrays with proper dimensions) to NPZ files\n",
    "    print(\"\\nSaving reshaped fields as individual .npz files...\")\n",
    "    output_dir = processed_data_folder / 'raw_fields'\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    saved_files = save_stacked_fields_as_npz(reshaped_fields, frame_to_index, output_dir, config_dir)\n",
    "    \n",
    "    # Also save the original stacked fields if needed (as backup)\n",
    "    save_original = False  # Set to True if you want to save both versions\n",
    "    if save_original:\n",
    "        print(\"\\nSaving original stacked fields (flattened) as individual .npz files...\")\n",
    "        original_output_dir = processed_data_folder / 'flattened_fields'\n",
    "        print(f\"Original output directory: {original_output_dir}\")\n",
    "        saved_original_files = save_stacked_fields_as_npz(stacked_fields, frame_to_index, original_output_dir, config_dir)\n",
    "    \n",
    "    print(f\"\\nProcessing completed for {config_dir.name}\")\n",
    "    print(f\"NPZ files saved to: {output_dir}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Process all VTK collections sequentially\n",
    "def process_all_vtk_collections(collections, max_files_to_process=None):\n",
    "    \"\"\"Process all VTK collections sequentially\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for collection in collections:\n",
    "        config_name = collection['config_dir'].name\n",
    "        start_time = time.time()\n",
    "        success = process_vtk_collection(collection, max_files_to_process)\n",
    "        end_time = time.time()\n",
    "        results[config_name] = {\n",
    "            'success': success,\n",
    "            'processing_time': end_time - start_time\n",
    "        }\n",
    "    \n",
    "    # Print summary of processing results\n",
    "    print(\"\\nProcessing Summary:\")\n",
    "    print(\"-\" * 80)\n",
    "    for config_name, result in results.items():\n",
    "        status = \"✓ Success\" if result['success'] else \"✗ Failed\"\n",
    "        time_taken = str(timedelta(seconds=int(result['processing_time'])))\n",
    "        print(f\"{status} - {config_name} - Time: {time_taken}\")\n",
    "    \n",
    "    # Count successes and failures\n",
    "    successes = sum(1 for result in results.values() if result['success'])\n",
    "    failures = len(results) - successes\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Total: {len(results)} collections processed, {successes} succeeded, {failures} failed\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Set to None to process all files in each collection\n",
    "max_files_to_process = None\n",
    "\n",
    "# Process all VTK collections\n",
    "if len(vtk_collections) > 0:\n",
    "    print(f\"Starting automatic processing of {len(vtk_collections)} VTK collections\")\n",
    "    processing_results = process_all_vtk_collections(vtk_collections, max_files_to_process)\n",
    "else:\n",
    "    print(\"No VTK collections found to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7334aaa9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has automatically searched for and processed all VTK files recursively, organizing them into structured NPZ arrays for further analysis. The key changes in this version are:\n",
    "\n",
    "1. **Recursive Search**: Starting from one directory above the execution point, the script finds all VTK file collections\n",
    "2. **Config-Based Organization**: Each configuration's data is stored in its parent directory\n",
    "3. **Automatic Processing**: All VTK collections are processed sequentially with detailed reporting\n",
    "\n",
    "Each configuration's data is stored in its own folder structure:\n",
    "```\n",
    "pipe_config_directory/\n",
    "    processed_data/\n",
    "        raw_fields/\n",
    "            frame_mapping.npz\n",
    "            dimensions.npz\n",
    "            rho.npz\n",
    "            u_x.npz\n",
    "            u_y.npz\n",
    "            u_magnitude.npz\n",
    "            wss_magnitude.npz\n",
    "            wall_mask.npz\n",
    "            ...\n",
    "```\n",
    "\n",
    "The processed data is now ready for use in the visualization and analysis notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
